{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9874a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                                                                            #\n",
    "#  Code for the USENIX Security '22 paper:                                   #\n",
    "#  How Machine Learning Is Solving the Binary Function Similarity Problem.   #\n",
    "#                                                                            #\n",
    "#  MIT License                                                               #\n",
    "#                                                                            #\n",
    "#  Copyright (c) 2019-2022 Cisco Talos                                       #\n",
    "#                                                                            #\n",
    "#  Permission is hereby granted, free of charge, to any person obtaining     #\n",
    "#  a copy of this software and associated documentation files (the           #\n",
    "#  \"Software\"), to deal in the Software without restriction, including       #\n",
    "#  without limitation the rights to use, copy, modify, merge, publish,       #\n",
    "#  distribute, sublicense, and/or sell copies of the Software, and to        #\n",
    "#  permit persons to whom the Software is furnished to do so, subject to     #\n",
    "#  the following conditions:                                                 #\n",
    "#                                                                            #\n",
    "#  The above copyright notice and this permission notice shall be            #\n",
    "#  included in all copies or substantial portions of the Software.           #\n",
    "#                                                                            #\n",
    "#  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,           #\n",
    "#  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF        #\n",
    "#  MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND                     #\n",
    "#  NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE    #\n",
    "#  LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION    #\n",
    "#  OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION     #\n",
    "#  WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.           #\n",
    "#                                                                            #\n",
    "#  Convert Catalog1 results                                                  #\n",
    "#                                                                            #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latter-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dutch-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(ff1, ff2):\n",
    "    return len(set(ff1) & set(ff2)) / len(set(ff1) | set(ff2))\n",
    "\n",
    "\n",
    "def compute_catalog1_similarity(df_input):\n",
    "    scores = list()\n",
    "    for idx, row in df_input.iterrows():\n",
    "        score = jaccard_similarity(row['catalog_1'].split(\";\"),\n",
    "                                   row['catalog_2'].split(\";\"))\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def compute_fuzzy_similarity(df_input, df_catalog):\n",
    "    df_input = df_input.merge(df_catalog,\n",
    "                              how='left',\n",
    "                              left_on=['idb_path_1', 'fva_1'],\n",
    "                              right_on=['path', 'address'])\n",
    "\n",
    "    df_input.rename(columns={'catalog_hash_list': 'catalog_1'}, inplace=True)\n",
    "    df_input.rename(columns={'time': 'catalog_time_1'}, inplace=True)\n",
    "\n",
    "    df_input = df_input.merge(df_catalog,\n",
    "                              how='left',\n",
    "                              left_on=['idb_path_2', 'fva_2'],\n",
    "                              right_on=['path', 'address'])\n",
    "\n",
    "    df_input.rename(columns={'catalog_hash_list': 'catalog_2'}, inplace=True)\n",
    "    df_input.rename(columns={'time': 'catalog_time_2'}, inplace=True)\n",
    "\n",
    "    df_input['sim'] = compute_catalog1_similarity(df_input)\n",
    "    df_input = df_input[['idb_path_1','fva_1','idb_path_2','fva_2','sim']]\n",
    "    return df_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5504354",
   "metadata": {},
   "source": [
    "### Process Dataset-1 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe484fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing ../data/raw_results/Catalog1/Dataset-1/Dataset-1_catalog1_16.csv\n",
      "[D] Processing ../data/raw_results/Catalog1/Dataset-1/Dataset-1_catalog1_64.csv\n",
      "[D] Processing ../data/raw_results/Catalog1/Dataset-1/Dataset-1_catalog1_128.csv\n",
      "[D] Processing ../data/raw_results/Catalog1/Dataset-1/Dataset-1_catalog1_32.csv\n"
     ]
    }
   ],
   "source": [
    "DB1_PATH = \"../../DBs/Dataset-1/pairs/testing/\"\n",
    "CATALOG1_PATH = \"../data/raw_results/Catalog1/Dataset-1\"\n",
    "\n",
    "for csv_name in os.listdir(CATALOG1_PATH):\n",
    "    if not csv_name.endswith(\".csv\"):\n",
    "        continue\n",
    "    csv_path = os.path.join(CATALOG1_PATH, csv_name)\n",
    "    print(\"[D] Processing {}\".format(csv_path))\n",
    "    \n",
    "    df_catalog = pd.read_csv(csv_path)\n",
    "    df_catalog.drop(df_catalog[df_catalog['catalog_hash_list'] == 'catalog_hash_list'].index, inplace=True)\n",
    "    df_catalog.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    df_pos = pd.read_csv(os.path.join(DB1_PATH, \"pos_testing_Dataset-1.csv\"), index_col=0)\n",
    "    df_neg = pd.read_csv(os.path.join(DB1_PATH, \"neg_testing_Dataset-1.csv\"), index_col=0)\n",
    "    df_pos_rank = pd.read_csv(os.path.join(DB1_PATH, \"pos_rank_testing_Dataset-1.csv\"), index_col=0)\n",
    "    df_neg_rank = pd.read_csv(os.path.join(DB1_PATH, \"neg_rank_testing_Dataset-1.csv\"), index_col=0)\n",
    "    \n",
    "    df_pos = compute_fuzzy_similarity(df_pos, df_catalog)\n",
    "    df_neg = compute_fuzzy_similarity(df_neg, df_catalog)\n",
    "    df_pos_rank = compute_fuzzy_similarity(df_pos_rank, df_catalog)\n",
    "    df_neg_rank = compute_fuzzy_similarity(df_neg_rank, df_catalog)\n",
    "        \n",
    "    df_pos.to_csv(\"../data/Dataset-1/pos_testing_{}\".format(csv_name), index=False)\n",
    "    df_neg.to_csv(\"../data/Dataset-1/neg_testing_{}\".format(csv_name), index=False)\n",
    "    df_pos_rank.to_csv(\"../data/Dataset-1/pos_rank_testing_{}\".format(csv_name), index=False)\n",
    "    df_neg_rank.to_csv(\"../data/Dataset-1/neg_rank_testing_{}\".format(csv_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e5a2cb",
   "metadata": {},
   "source": [
    "### Process Dataset-2 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "outstanding-peace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing ../data/raw_results/Catalog1/Dataset-2/Dataset-2_catalog1_128.csv\n",
      "[D] Processing ../data/raw_results/Catalog1/Dataset-2/Dataset-2_catalog1_32.csv\n",
      "[D] Processing ../data/raw_results/Catalog1/Dataset-2/Dataset-2_catalog1_16.csv\n",
      "[D] Processing ../data/raw_results/Catalog1/Dataset-2/Dataset-2_catalog1_64.csv\n"
     ]
    }
   ],
   "source": [
    "DB2_PATH = \"../../DBs/Dataset-2/pairs/\"\n",
    "CATALOG1_PATH = \"../data/raw_results/Catalog1/Dataset-2\"\n",
    "\n",
    "for csv_name in os.listdir(CATALOG1_PATH):\n",
    "    if not csv_name.endswith(\".csv\"):\n",
    "        continue\n",
    "    csv_path = os.path.join(CATALOG1_PATH, csv_name)\n",
    "    print(\"[D] Processing {}\".format(csv_path))\n",
    "    \n",
    "    df_catalog = pd.read_csv(csv_path)\n",
    "    df_catalog.drop(df_catalog[df_catalog['catalog_hash_list'] == 'catalog_hash_list'].index, inplace=True)\n",
    "    df_catalog.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    df_pos = pd.read_csv(os.path.join(DB2_PATH, \"pos_testing_Dataset-2.csv\"), index_col=0)\n",
    "    df_neg = pd.read_csv(os.path.join(DB2_PATH, \"neg_testing_Dataset-2.csv\"), index_col=0)\n",
    "    df_pos_rank = pd.read_csv(os.path.join(DB2_PATH, \"pos_rank_testing_Dataset-2.csv\"), index_col=0)\n",
    "    df_neg_rank = pd.read_csv(os.path.join(DB2_PATH, \"neg_rank_testing_Dataset-2.csv\"), index_col=0)\n",
    "    \n",
    "    df_pos = compute_fuzzy_similarity(df_pos, df_catalog)\n",
    "    df_neg = compute_fuzzy_similarity(df_neg, df_catalog)\n",
    "    df_pos_rank = compute_fuzzy_similarity(df_pos_rank, df_catalog)\n",
    "    df_neg_rank = compute_fuzzy_similarity(df_neg_rank, df_catalog)\n",
    "        \n",
    "    df_pos.to_csv(\"../data/Dataset-2/pos_testing_{}\".format(csv_name), index=False)\n",
    "    df_neg.to_csv(\"../data/Dataset-2/neg_testing_{}\".format(csv_name), index=False)\n",
    "    df_pos_rank.to_csv(\"../data/Dataset-2/pos_rank_testing_{}\".format(csv_name), index=False)\n",
    "    df_neg_rank.to_csv(\"../data/Dataset-2/neg_rank_testing_{}\".format(csv_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67bc08a",
   "metadata": {},
   "source": [
    "### Process Dataset-Vulnerability results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d73893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing ../data/raw_results/Catalog1/Dataset-Vulnerability/Dataset-Vulnerability_catalog1_128.csv\n",
      "[D] Processing ../data/raw_results/Catalog1/Dataset-Vulnerability/Dataset-Vulnerability_catalog1_32.csv\n",
      "[D] Processing ../data/raw_results/Catalog1/Dataset-Vulnerability/Dataset-Vulnerability_catalog1_16.csv\n",
      "[D] Processing ../data/raw_results/Catalog1/Dataset-Vulnerability/Dataset-Vulnerability_catalog1_64.csv\n"
     ]
    }
   ],
   "source": [
    "DB2_PATH = \"../../DBs/Dataset-Vulnerability/pairs/\"\n",
    "CATALOG1_PATH = \"../data/raw_results/Catalog1/Dataset-Vulnerability/\"\n",
    "\n",
    "for csv_name in os.listdir(CATALOG1_PATH):\n",
    "    if not csv_name.endswith(\".csv\"):\n",
    "        continue\n",
    "    csv_path = os.path.join(CATALOG1_PATH, csv_name)\n",
    "    print(\"[D] Processing {}\".format(csv_path))\n",
    "    \n",
    "    df_catalog = pd.read_csv(csv_path)\n",
    "    df_catalog.drop(df_catalog[df_catalog['catalog_hash_list'] == 'catalog_hash_list'].index, inplace=True)\n",
    "    df_catalog.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    df_testing = pd.read_csv(os.path.join(DB2_PATH, \"pairs_testing_Dataset-Vulnerability.csv\"), index_col=0)\n",
    "    \n",
    "    df_testing = compute_fuzzy_similarity(df_testing, df_catalog)\n",
    "        \n",
    "    df_testing.to_csv(\"../data/Dataset-Vulnerability/testing_{}\".format(csv_name), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
