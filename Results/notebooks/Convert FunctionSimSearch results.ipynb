{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d69932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                                                                            #\n",
    "#  Code for the USENIX Security '22 paper:                                   #\n",
    "#  How Machine Learning Is Solving the Binary Function Similarity Problem.   #\n",
    "#                                                                            #\n",
    "#  MIT License                                                               #\n",
    "#                                                                            #\n",
    "#  Copyright (c) 2019-2022 Cisco Talos                                       #\n",
    "#                                                                            #\n",
    "#  Permission is hereby granted, free of charge, to any person obtaining     #\n",
    "#  a copy of this software and associated documentation files (the           #\n",
    "#  \"Software\"), to deal in the Software without restriction, including       #\n",
    "#  without limitation the rights to use, copy, modify, merge, publish,       #\n",
    "#  distribute, sublicense, and/or sell copies of the Software, and to        #\n",
    "#  permit persons to whom the Software is furnished to do so, subject to     #\n",
    "#  the following conditions:                                                 #\n",
    "#                                                                            #\n",
    "#  The above copyright notice and this permission notice shall be            #\n",
    "#  included in all copies or substantial portions of the Software.           #\n",
    "#                                                                            #\n",
    "#  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,           #\n",
    "#  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF        #\n",
    "#  MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND                     #\n",
    "#  NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE    #\n",
    "#  LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION    #\n",
    "#  OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION     #\n",
    "#  WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.           #\n",
    "#                                                                            #\n",
    "#  Convert FunctionSimSearch results                                         #\n",
    "#                                                                            #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "convinced-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "historic-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_dist(n1, n2):\n",
    "    x = n1 ^ n2\n",
    "    setBits = 0\n",
    "\n",
    "    while (x > 0):\n",
    "        setBits += x & 1\n",
    "        x >>= 1\n",
    "\n",
    "    return setBits\n",
    "\n",
    "\n",
    "def hamming_similarity_s(t1, t2):\n",
    "    diff = hamming_dist(t1[0], t2[0]) + hamming_dist(t1[1], t2[1])\n",
    "    return 1 - (diff / 128.0)\n",
    "\n",
    "\n",
    "def compute_fss_similarity(df_input):\n",
    "    scores = list()\n",
    "    for idx, row in tqdm(df_input.iterrows()):\n",
    "        score = hamming_similarity_s(tuple(row[['hashes0_1', 'hashes1_1']].values),\n",
    "                                     tuple(row[['hashes0_2', 'hashes1_2']].values))\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collectible-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fuzzy_similarity(df_pairs, df_fss):\n",
    "    df_pairs = df_pairs.merge(df_fss,\n",
    "                              how='left',\n",
    "                              left_on=['idb_path_1', 'fva_1'],\n",
    "                              right_on=['path', 'address'])\n",
    "    df_pairs.rename(columns={'hashes0': 'hashes0_1',\n",
    "                             'hashes1': 'hashes1_1'\n",
    "                             }, inplace=True)\n",
    "    df_pairs.rename(columns={'time': 'fss_time_1'}, inplace=True)\n",
    "\n",
    "    df_pairs = df_pairs.merge(df_fss,\n",
    "                              how='left',\n",
    "                              left_on=['idb_path_2', 'fva_2'],\n",
    "                              right_on=['path', 'address'])\n",
    "    df_pairs.rename(columns={'hashes0': 'hashes0_2',\n",
    "                             'hashes1': 'hashes1_2'\n",
    "                             }, inplace=True)\n",
    "    df_pairs.rename(columns={'time': 'fss_time_2'}, inplace=True)\n",
    "\n",
    "    df_pairs['sim'] = compute_fss_similarity(df_pairs)\n",
    "    df_pairs = df_pairs[['idb_path_1','fva_1','idb_path_2','fva_2','sim']]\n",
    "    return df_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd9dc0",
   "metadata": {},
   "source": [
    "### Process Dataset-1 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c547ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing IMM:0.00_MNEM:0.00_GRAPH:1.00.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450000it [01:55, 3911.92it/s]\n",
      "450000it [01:52, 3999.31it/s]\n",
      "800it [00:00, 3970.95it/s]\n",
      "80000it [00:20, 3977.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing IMM:0.00_MNEM:1.00_GRAPH:1.00.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450000it [01:53, 3959.27it/s]\n",
      "450000it [01:52, 4017.14it/s]\n",
      "800it [00:00, 4129.07it/s]\n",
      "80000it [00:19, 4135.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing IMM:4.00_MNEM:0.05_GRAPH:1.00.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450000it [01:49, 4111.88it/s]\n",
      "450000it [01:53, 3959.48it/s]\n",
      "800it [00:00, 3920.85it/s]\n",
      "80000it [00:20, 3909.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing IMM:1.00_MNEM:1.00_GRAPH:1.00.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450000it [01:53, 3961.70it/s]\n",
      "450000it [01:51, 4030.62it/s]\n",
      "800it [00:00, 4113.12it/s]\n",
      "80000it [00:19, 4135.52it/s]\n"
     ]
    }
   ],
   "source": [
    "DB1_PATH = \"../../DBs/Dataset-1/pairs/testing/\"\n",
    "FSS_PATH = \"../data/raw_results/FunctionSimSearch/Dataset-1\"\n",
    "\n",
    "for csv_name in [x for x in os.listdir(FSS_PATH) if x.endswith(\".csv\")]:\n",
    "    csv_path = os.path.join(FSS_PATH, csv_name)\n",
    "    print(\"[D] Processing {}\".format(csv_name))\n",
    "\n",
    "    df_pos = pd.read_csv(os.path.join(DB1_PATH, \"pos_testing_Dataset-1.csv\"), index_col=0)\n",
    "    df_neg = pd.read_csv(os.path.join(DB1_PATH, \"neg_testing_Dataset-1.csv\"), index_col=0)\n",
    "    df_pos_rank = pd.read_csv(os.path.join(DB1_PATH, \"pos_rank_testing_Dataset-1.csv\"), index_col=0)\n",
    "    df_neg_rank = pd.read_csv(os.path.join(DB1_PATH, \"neg_rank_testing_Dataset-1.csv\"), index_col=0)\n",
    "\n",
    "    df_fss = pd.read_csv(csv_path)\n",
    "    df_fss.drop(df_fss[df_fss['branching_nodes'] == 'branching_nodes'].index, inplace=True)\n",
    "    df_fss.reset_index(inplace=True, drop=True)\n",
    "    df_fss = df_fss.astype({'hashes0': np.uint64, 'hashes1': np.uint64})\n",
    "\n",
    "    df_pos = compute_fuzzy_similarity(df_pos, df_fss)\n",
    "    df_neg = compute_fuzzy_similarity(df_neg, df_fss)\n",
    "    df_pos_rank = compute_fuzzy_similarity(df_pos_rank, df_fss)\n",
    "    df_neg_rank = compute_fuzzy_similarity(df_neg_rank, df_fss)\n",
    "\n",
    "    df_pos.to_csv(\"../data/Dataset-1/pos_testing_Dataset-1_{}\".format(csv_name), index=False)\n",
    "    df_neg.to_csv(\"../data/Dataset-1/neg_testing_Dataset-1_{}\".format(csv_name), index=False)\n",
    "    df_pos_rank.to_csv(\"../data/Dataset-1/pos_rank_testing_Dataset-1_{}\".format(csv_name), index=False)\n",
    "    df_neg_rank.to_csv(\"../data/Dataset-1/neg_rank_testing_Dataset-1_{}\".format(csv_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b844a",
   "metadata": {},
   "source": [
    "### Process Dataset-2 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sitting-pressure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing IMM:0.00_MNEM:0.00_GRAPH:1.00.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [00:36, 4127.75it/s]\n",
      "150000it [00:36, 4143.63it/s]\n",
      "600it [00:00, 4129.16it/s]\n",
      "60000it [00:14, 4143.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing IMM:0.00_MNEM:1.00_GRAPH:1.00.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [00:36, 4109.16it/s]\n",
      "150000it [00:36, 4125.44it/s]\n",
      "600it [00:00, 4072.02it/s]\n",
      "60000it [00:14, 4126.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing IMM:4.00_MNEM:0.05_GRAPH:1.00.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [00:36, 4131.54it/s]\n",
      "150000it [00:36, 4118.87it/s]\n",
      "600it [00:00, 4119.01it/s]\n",
      "60000it [00:14, 4124.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing IMM:1.00_MNEM:1.00_GRAPH:1.00.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [00:36, 4111.05it/s]\n",
      "150000it [00:36, 4109.43it/s]\n",
      "600it [00:00, 4101.77it/s]\n",
      "60000it [00:14, 4120.22it/s]\n"
     ]
    }
   ],
   "source": [
    "DB2_PATH = \"../../DBs/Dataset-2/pairs/\"\n",
    "FSS_PATH = \"../data/raw_results/FunctionSimSearch/Dataset-2\"\n",
    "\n",
    "for csv_name in [x for x in os.listdir(FSS_PATH) if x.endswith(\".csv\")]:\n",
    "    csv_path = os.path.join(FSS_PATH, csv_name)\n",
    "    print(\"[D] Processing {}\".format(csv_name))\n",
    "\n",
    "    df_pos = pd.read_csv(os.path.join(DB2_PATH, \"pos_testing_Dataset-2.csv\"), index_col=0)\n",
    "    df_neg = pd.read_csv(os.path.join(DB2_PATH, \"neg_testing_Dataset-2.csv\"), index_col=0)\n",
    "    df_pos_rank = pd.read_csv(os.path.join(DB2_PATH, \"pos_rank_testing_Dataset-2.csv\"), index_col=0)\n",
    "    df_neg_rank = pd.read_csv(os.path.join(DB2_PATH, \"neg_rank_testing_Dataset-2.csv\"), index_col=0)\n",
    "\n",
    "    df_fss = pd.read_csv(csv_path)\n",
    "    df_fss.drop(df_fss[df_fss['branching_nodes'] == 'branching_nodes'].index, inplace=True)\n",
    "    df_fss.reset_index(inplace=True, drop=True)\n",
    "    df_fss = df_fss.astype({'hashes0': np.uint64, 'hashes1': np.uint64})\n",
    "\n",
    "    df_pos = compute_fuzzy_similarity(df_pos, df_fss)\n",
    "    df_neg = compute_fuzzy_similarity(df_neg, df_fss)\n",
    "    df_pos_rank = compute_fuzzy_similarity(df_pos_rank, df_fss)\n",
    "    df_neg_rank = compute_fuzzy_similarity(df_neg_rank, df_fss)\n",
    "\n",
    "    df_pos.to_csv(\"../data/Dataset-2/pos_testing_Dataset-2_{}\".format(csv_name), index=False)\n",
    "    df_neg.to_csv(\"../data/Dataset-2/neg_testing_Dataset-2_{}\".format(csv_name), index=False)\n",
    "    df_pos_rank.to_csv(\"../data/Dataset-2/pos_rank_testing_Dataset-2_{}\".format(csv_name), index=False)\n",
    "    df_neg_rank.to_csv(\"../data/Dataset-2/neg_rank_testing_Dataset-2_{}\".format(csv_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db574cbb",
   "metadata": {},
   "source": [
    "### Process Dataset-Vulnerability results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11aa5ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs_testing_Dataset-Vulnerability.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../../DBs/Dataset-Vulnerability/pairs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a596941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing IMM:0.00_MNEM:0.00_GRAPH:1.00.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88700it [00:21, 4111.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing IMM:0.00_MNEM:1.00_GRAPH:1.00.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88700it [00:21, 4104.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing IMM:4.00_MNEM:0.05_GRAPH:1.00.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88700it [00:21, 4111.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Processing IMM:1.00_MNEM:1.00_GRAPH:1.00.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88700it [00:21, 4098.62it/s]\n"
     ]
    }
   ],
   "source": [
    "DB2_PATH = \"../../DBs/Dataset-Vulnerability/pairs/\"\n",
    "FSS_PATH = \"../data/raw_results/FunctionSimSearch/Dataset-Vulnerability/\"\n",
    "\n",
    "for csv_name in [x for x in os.listdir(FSS_PATH) if x.endswith(\".csv\")]:\n",
    "    csv_path = os.path.join(FSS_PATH, csv_name)\n",
    "    print(\"[D] Processing {}\".format(csv_name))\n",
    "\n",
    "    df_testing = pd.read_csv(os.path.join(DB2_PATH, \"pairs_testing_Dataset-Vulnerability.csv\"), index_col=0)\n",
    "\n",
    "    df_fss = pd.read_csv(csv_path)\n",
    "    df_fss.drop(df_fss[df_fss['branching_nodes'] == 'branching_nodes'].index, inplace=True)\n",
    "    df_fss.reset_index(inplace=True, drop=True)\n",
    "    df_fss = df_fss.astype({'hashes0': np.uint64, 'hashes1': np.uint64})\n",
    "\n",
    "    df_testing = compute_fuzzy_similarity(df_testing, df_fss)\n",
    "\n",
    "    df_testing.to_csv(\"../data/Dataset-Vulnerability/testing_Dataset-Vulnerability_{}\".format(csv_name), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
