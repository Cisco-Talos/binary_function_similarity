diff -Nru ../gensim-3.8.3/gensim/models/__init__.py ./gensim/models/__init__.py
--- ../gensim-3.8.3/gensim/models/__init__.py	2020-05-03 01:52:10.000000000 +0200
+++ ./gensim/models/__init__.py	2022-08-03 16:25:12.000000000 +0200
@@ -13,6 +13,7 @@
 from .logentropy_model import LogEntropyModel  # noqa:F401
 from .word2vec import Word2Vec  # noqa:F401
 from .doc2vec import Doc2Vec  # noqa:F401
+from .asm2vec import Asm2Vec  # noqa:F401
 from .keyedvectors import KeyedVectors, WordEmbeddingSimilarityIndex  # noqa:F401
 from .ldamulticore import LdaMulticore  # noqa:F401
 from .phrases import Phrases  # noqa:F401
diff -Nru ../gensim-3.8.3/gensim/models/asm2vec.py ./gensim/models/asm2vec.py
--- ../gensim-3.8.3/gensim/models/asm2vec.py	1970-01-01 01:00:00.000000000 +0100
+++ ./gensim/models/asm2vec.py	2022-08-03 16:25:12.000000000 +0200
@@ -0,0 +1,1183 @@
+import logging
+import os
+import warnings
+
+try:
+    from queue import Queue
+except ImportError:
+    from Queue import Queue  # noqa:F401
+
+from collections import namedtuple, defaultdict
+try:
+    from collections.abc import Iterable
+except ImportError:
+    from collections import Iterable
+from timeit import default_timer
+
+from numpy import zeros, float32 as REAL, empty, ones, \
+    memmap as np_memmap, vstack, integer, dtype
+
+from gensim.utils import call_on_class_only, deprecated
+from gensim import utils, matutils  # utility fnc for pickling, common scipy operations etc
+from gensim.models.doc2vec import TaggedDocument, Doctag
+from gensim.models.word2vec import Word2VecKeyedVectors, Word2VecVocab, Word2VecTrainables
+from gensim.models.word2vec import train_cbow_pair, train_sg_pair, train_batch_sg  # noqa
+from six.moves import range
+from six import string_types, integer_types, itervalues
+from gensim.models.base_any2vec import BaseWordEmbeddingsModel
+from gensim.models.keyedvectors import Doc2VecKeyedVectors
+from types import GeneratorType
+
+logger = logging.getLogger(__name__)
+
+try:
+    from gensim.models.doc2vec_inner import train_document_dm_asm2vec
+except ImportError:
+    raise utils.NO_CYTHON
+
+# try:
+#     from gensim.models.doc2vec_corpusfile import (
+#         d2v_train_epoch_dm,
+#         CORPUSFILE_VERSION
+#     )
+# except ImportError:
+#     # corpusfile doc2vec is not supported
+#     CORPUSFILE_VERSION = -1
+
+#     def d2v_train_epoch_dm(model, corpus_file, offset, start_doctag, _cython_vocab, _cur_epoch, _expected_examples,
+#                            _expected_words, work, _neu1, docvecs_count, word_vectors=None, word_locks=None,
+#                            learn_doctags=True, learn_words=True, learn_hidden=True, doctag_vectors=None,
+#                            doctag_locks=None):
+#         raise NotImplementedError("Training with corpus_file argument is not supported.")
+
+
+
+
+class Function(namedtuple('Function', 'words tags')):
+    """Represents a document along with a tag, input document format for :class:`~gensim.models.asm2vec.Asm2Vec`.
+
+    A single document, made up of `words` (a list of unicode string tokens) and `tags` (a list of tokens).
+    Tags may be one or more unicode string tokens, but typical practice (which will also be the most memory-efficient)
+    is for the tags list to include a unique integer id as the only tag.
+
+    Replaces "sentence as a list of words" from :class:`gensim.models.word2vec.Word2Vec`.
+
+    """
+    def __str__(self):
+        """Human readable representation of the object's state, used for debugging.
+
+        Returns
+        -------
+        str
+           Human readable representation of the object's state (words and tags).
+
+        """
+        return '%s(%s, %s)' % (self.__class__.__name__, self.words, self.tags)
+
+    def get_tokens(self):
+        tokens = list()
+        for word in self.words:
+            if isinstance(word, Instruction):
+                tokens.extend(word.get_tokens())
+            if isinstance(word, list):
+                tokens.extend(word)    
+            if isinstance(word, string_types):
+                tokens.append(word)
+        return tokens
+
+
+class Instruction(namedtuple('Instruction', 'operator operands')):
+    """Represents an instruction along with an operator and operands."""
+
+    def __str__(self):
+        """Human readable representation of the object's state, used for debugging.
+
+        Returns
+        -------
+        str
+           Human readable representation of the object's state (words and tags).
+
+        """
+        return '%s(%s, %s)' % (self.__class__.__name__, self.operator, self.operands)
+
+    def get_tokens(self):
+        return [self.operator] + self.operands
+
+
+class Asm2Vec(BaseWordEmbeddingsModel):
+    """Class for training, using and evaluating neural networks described in
+    `Distributed Representations of Sentences and Documents <http://arxiv.org/abs/1405.4053v2>`_.
+
+    Some important internal attributes are the following:
+
+    Attributes
+    ----------
+    wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`
+        This object essentially contains the mapping between words and embeddings. After training, it can be used
+        directly to query those embeddings in various ways. See the module level docstring for examples.
+
+    docvecs : :class:`~gensim.models.keyedvectors.Doc2VecKeyedVectors`
+        This object contains the paragraph vectors learned from the training data. There will be one such vector
+        for each unique document tag supplied during training. They may be individually accessed using the tag
+        as an indexed-access key. For example, if one of the training documents used a tag of 'doc003':
+
+        .. sourcecode:: pycon
+
+            >>> model.docvecs['doc003']
+
+    vocabulary : :class:`~gensim.models.doc2vec.Asm2VecVocab`
+        This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.
+        Besides keeping track of all unique words, this object provides extra functionality, such as
+        sorting words by frequency, or discarding extremely rare words.
+
+    trainables : :class:`~gensim.models.doc2vec.Asm2VecTrainables`
+        This object represents the inner shallow neural network used to train the embeddings. The semantics of the
+        network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with
+        a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings
+        The only addition to the underlying NN used in :class:`~gensim.models.word2vec.Word2Vec` is that the input
+        includes not only the word vectors of each word in the context, but also the paragraph vector.
+
+    """
+    def __init__(self, documents=None, corpus_file=None, dm_mean=None, docvecs=None, docvecs_mapfile=None, comment=None, trim_rule=None, callbacks=(),
+                 **kwargs):
+        """
+
+        Parameters
+        ----------
+        documents : iterable of list of :class:`~gensim.models.asm2vec.Function`, optional
+            Input corpus, can be simply a list of elements, but for larger corpora,consider an iterable that streams
+            the documents directly from disk/network. If you don't supply `documents` (or `corpus_file`), the model is
+            left uninitialized -- use if you plan to initialize it in some other way.
+        corpus_file : str, optional
+            Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.
+            You may use this argument instead of `documents` to get performance boost. Only one of `documents` or
+            `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).
+            Documents' tags are assigned automatically and are equal to line number, as in
+            :class:`~gensim.models.doc2vec.TaggedLineDocument`.
+        vector_size : int, optional
+            Dimensionality of the feature vectors.
+        window : int, optional
+            The maximum distance between the current and predicted word within a sentence.
+        alpha : float, optional
+            The initial learning rate.
+        min_alpha : float, optional
+            Learning rate will linearly drop to `min_alpha` as training progresses.
+        seed : int, optional
+            Seed for the random number generator. Initial vectors for each word are seeded with a hash of
+            the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,
+            you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter
+            from OS thread scheduling.
+            In Python 3, reproducibility between interpreter launches also requires use of the `PYTHONHASHSEED`
+            environment variable to control hash randomization.
+        min_count : int, optional
+            Ignores all words with total frequency lower than this.
+        max_vocab_size : int, optional
+            Limits the RAM during vocabulary building; if there are more unique
+            words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.
+            Set to `None` for no limit.
+        sample : float, optional
+            The threshold for configuring which higher-frequency words are randomly downsampled,
+            useful range is (0, 1e-5).
+        workers : int, optional
+            Use these many worker threads to train the model (=faster training with multicore machines).
+        epochs : int, optional
+            Number of iterations (epochs) over the corpus.
+        hs : {1,0}, optional
+            If 1, hierarchical softmax will be used for model training.
+            If set to 0, and `negative` is non-zero, negative sampling will be used.
+        negative : int, optional
+            If > 0, negative sampling will be used, the int for negative specifies how many "noise words"
+            should be drawn (usually between 5-20).
+            If set to 0, no negative sampling is used.
+        ns_exponent : float, optional
+            The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion
+            to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more
+            than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.
+            More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that
+            other values may perform better for recommendation applications.
+        dm_mean : {1,0}, optional
+            If 0 , use the sum of the context word vectors. If 1, use the mean.
+            Only applies when `dm` is used in non-concatenative mode.
+        trim_rule : function, optional
+            Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,
+            be trimmed away, or handled using the default (discard if word count < min_count).
+            Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),
+            or a callable that accepts parameters (word, count, min_count) and returns either
+            :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.
+            The rule, if given, is only used to prune vocabulary during current method call and is not stored as part
+            of the model.
+
+            The input parameters are of the following types:
+                * `word` (str) - the word we are examining
+                * `count` (int) - the word's frequency count in the corpus
+                * `min_count` (int) - the minimum count threshold.
+
+        callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional
+            List of callbacks that need to be executed/run at specific stages during training.
+
+        """
+
+        super(Asm2Vec, self).__init__(
+            # NOTE: If 1, skip-gram is used, otherwise, CBOW is employed.
+            sg=(1 + 1) % 2,
+            callbacks=callbacks,
+            **kwargs)
+
+        self.load = call_on_class_only
+
+        if dm_mean is not None:
+            self.cbow_mean = dm_mean
+
+        vocabulary_keys = ['max_vocab_size', 'min_count', 'sample', 'sorted_vocab', 'null_word', 'ns_exponent']
+        vocabulary_kwargs = dict((k, kwargs[k]) for k in vocabulary_keys if k in kwargs)
+        self.vocabulary = Asm2VecVocab(**vocabulary_kwargs)
+
+        trainables_keys = ['seed', 'hashfxn', 'window']
+        trainables_kwargs = dict((k, kwargs[k]) for k in trainables_keys if k in kwargs)
+        self.trainables = Asm2VecTrainables(
+            vector_size=self.vector_size, **trainables_kwargs)
+
+        self.quick_inference = False
+
+        self.wv = Word2VecKeyedVectors(self.vector_size)
+        self.docvecs = docvecs or Doc2VecKeyedVectors(self.vector_size * 2, docvecs_mapfile)
+
+        self.comment = comment
+
+        # NOTE: Let's start directly with the training
+        if documents is not None or corpus_file is not None:
+            self._check_input_data_sanity(data_iterable=documents, corpus_file=corpus_file)
+            if corpus_file is not None and not isinstance(corpus_file, string_types):
+                raise TypeError("You must pass string as the corpus_file argument.")
+            elif isinstance(documents, GeneratorType):
+                raise TypeError("You can't pass a generator as the documents argument. Try a sequence.")
+            self.build_vocab(documents=documents, corpus_file=corpus_file, trim_rule=trim_rule)
+            self.train(
+                documents=documents, corpus_file=corpus_file, total_examples=self.corpus_count,
+                total_words=self.corpus_total_words, epochs=self.epochs, start_alpha=self.alpha,
+                end_alpha=self.min_alpha, callbacks=callbacks)
+
+    @property
+    def dm(self):
+        """Indicates whether 'distributed memory' (PV-DM) will be used, else 'distributed bag of words'
+        (PV-DBOW) is used.
+
+        """
+        # NOTE: In Asm2Vec only PV-DM is defined, as it's a variant of that model.
+        return not self.sg  # opposite of SG
+
+    def _set_train_params(self, **kwargs):
+        pass
+
+    def _clear_post_train(self):
+        """Alias for :meth:`~gensim.models.asm2vec.Asm2Vec.clear_sims`."""
+        self.clear_sims()
+
+    def clear_sims(self):
+        """Resets the current word vectors. """
+        self.wv.vectors_norm = None
+        self.wv.vectors_docs_norm = None
+
+    def reset_from(self, other_model):
+        """Copy shareable data structures from another (possibly pre-trained) model.
+
+        Parameters
+        ----------
+        other_model : :class:`~gensim.models.asm2vec.Asm2Vec`
+            Other model whose internal data structures will be copied over to the current object.
+
+        """
+        self.wv.vocab = other_model.wv.vocab
+        self.wv.index2word = other_model.wv.index2word
+        self.vocabulary.cum_table = other_model.vocabulary.cum_table
+        self.corpus_count = other_model.corpus_count
+        self.docvecs.count = other_model.docvecs.count
+        self.docvecs.doctags = other_model.docvecs.doctags
+        self.docvecs.offset2doctag = other_model.docvecs.offset2doctag
+        self.trainables.reset_weights(self.hs, self.negative, self.wv, self.docvecs)
+
+    def reset_model_for_fast_inference(self, other_model, docvecs_count):
+        """Copy shareable data structures from another (possibly pre-trained) model.
+
+        Parameters
+        ----------
+        other_model : :class:`~gensim.models.asm2vec.Asm2Vec`
+            Other model whose internal data structures will be copied over to the current object.
+
+        """
+        self.wv = other_model.wv
+        self.vocabulary.cum_table = other_model.vocabulary.cum_table
+        self.corpus_count = other_model.corpus_count
+        self.docvecs.count = docvecs_count
+        self.docvecs.doctags = other_model.docvecs.doctags
+        self.docvecs.offset2doctag = other_model.docvecs.offset2doctag
+        self.trainables = other_model.trainables
+        self.trainables.reset_doc_weights(self.docvecs)
+        self.quick_inference = True
+        return
+
+    # def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,
+    #                     total_examples=None, total_words=None, offsets=None, start_doctags=None, **kwargs):
+    #     work, neu1 = thread_private_mem
+    #     doctag_vectors = self.docvecs.vectors_docs
+    #     doctag_locks = self.trainables.vectors_docs_lockf
+
+    #     offset = offsets[thread_id]
+    #     start_doctag = start_doctags[thread_id]
+
+    #     examples, tally, raw_tally = d2v_train_epoch_dm(
+    #         self, corpus_file, offset, start_doctag, cython_vocab, cur_epoch,
+    #         total_examples, total_words, work, neu1, self.docvecs.count,
+    #         doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
+
+    #     return examples, tally, raw_tally
+
+    def _get_thread_working_mem(self):
+        """Computes the memory used per worker thread.
+
+        Returns
+        -------
+        (np.ndarray, np.ndarray, np.ndarray)
+            Each worker threads private work memory.
+
+        """
+        work = matutils.zeros_aligned(self.trainables.layer1_size, dtype=REAL)  # per-thread private work memory
+        neu1 = matutils.zeros_aligned(self.trainables.layer1_size, dtype=REAL)
+        tmp_opnds = matutils.zeros_aligned(self.wv.vector_size, dtype=REAL)
+        return work, neu1, tmp_opnds
+
+
+    def _do_train_job(self, job, alpha, inits):
+        """Train model using `job` data.
+
+        Parameters
+        ----------
+        job : iterable of list of :class:`~gensim.models.asm2vec.Function`
+            The corpus chunk to be used for training this batch.
+        alpha : float
+            Learning rate to be used for training this batch.
+        inits : (np.ndarray, np.ndarray)
+            Each worker threads private work memory.
+
+        Returns
+        -------
+        (int, int)
+             2-tuple (effective word count after ignoring unknown words and sentence length trimming, total word count).
+
+        """
+        work, neu1, tmp_opnds = inits
+        tally = 0
+
+        # TODO: rename doc to function
+        for doc in job:
+            doctag_indexes = self.vocabulary.indexed_doctags(doc.tags, self.docvecs)
+            doctag_vectors = self.docvecs.vectors_docs
+            doctag_locks = self.trainables.vectors_docs_lockf
+            if self.quick_inference:
+                tally += train_document_dm_asm2vec(
+                    self, doc, doctag_indexes, alpha, work, neu1, tmp_opnds,
+                    learn_words=False, learn_hidden=False,
+                    doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
+                )
+            else:
+                tally += train_document_dm_asm2vec(
+                    self, doc, doctag_indexes, alpha, work, neu1, tmp_opnds,
+                    doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
+                )
+        return tally, self._raw_word_count(job)
+
+    def train(self, documents=None, corpus_file=None, total_examples=None, total_words=None,
+              epochs=None, start_alpha=None, end_alpha=None,
+              word_count=0, queue_factor=2, report_delay=1.0, callbacks=()):
+        """Update the model's neural weights.
+
+        To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate
+        progress-percentage logging, either `total_examples` (count of documents) or `total_words` (count of
+        raw words in documents) **MUST** be provided. If `documents` is the same corpus
+        that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,
+        you can simply use `total_examples=self.corpus_count`.
+
+        To avoid common mistakes around the model's ability to do multiple training passes itself, an
+        explicit `epochs` argument **MUST** be provided. In the common and recommended case
+        where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once,
+        you can set `epochs=self.iter`.
+
+        Parameters
+        ----------
+        documents : iterable of list of :class:`~gensim.models.asm2vec.Function`, optional
+            Can be simply a list of elements, but for larger corpora,consider an iterable that streams
+            the documents directly from disk/network. If you don't supply `documents` (or `corpus_file`), the model is
+            left uninitialized -- use if you plan to initialize it in some other way.
+        corpus_file : str, optional
+            Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.
+            You may use this argument instead of `documents` to get performance boost. Only one of `documents` or
+            `corpus_file` arguments need to be passed (not both of them). Documents' tags are assigned automatically
+            and are equal to line number, as in :class:`~gensim.models.doc2vec.TaggedLineDocument`.
+        total_examples : int, optional
+            Count of documents.
+        total_words : int, optional
+            Count of raw words in documents.
+        epochs : int, optional
+            Number of iterations (epochs) over the corpus.
+        start_alpha : float, optional
+            Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,
+            for this one call to `train`.
+            Use only if making multiple calls to `train`, when you want to manage the alpha learning-rate yourself
+            (not recommended).
+        end_alpha : float, optional
+            Final learning rate. Drops linearly from `start_alpha`.
+            If supplied, this replaces the final `min_alpha` from the constructor, for this one call to
+            :meth:`~gensim.models.asm2vec.Asm2Vec.train`.
+            Use only if making multiple calls to :meth:`~gensim.models.asm2vec.Asm2Vec.train`, when you want to manage
+            the alpha learning-rate yourself (not recommended).
+        word_count : int, optional
+            Count of words already trained. Set this to 0 for the usual
+            case of training on all words in documents.
+        queue_factor : int, optional
+            Multiplier for size of queue (number of workers * queue_factor).
+        report_delay : float, optional
+            Seconds to wait before reporting progress.
+        callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional
+            List of callbacks that need to be executed/run at specific stages during training.
+
+        """
+        kwargs = {}
+
+        if corpus_file is None and documents is None:
+            raise TypeError("Either one of corpus_file or documents value must be provided")
+
+        if corpus_file is not None and documents is not None:
+            raise TypeError("Both corpus_file and documents must not be provided at the same time")
+
+        if documents is None and not os.path.isfile(corpus_file):
+            raise TypeError("Parameter corpus_file must be a valid path to a file, got %r instead" % corpus_file)
+
+        if documents is not None and not isinstance(documents, Iterable):
+            raise TypeError("documents must be an iterable of list, got %r instead" % documents)
+
+        # if corpus_file is not None:
+        #     # Calculate offsets for each worker along with initial doctags (doctag ~ document/line number in a file)
+        #     offsets, start_doctags = self._get_offsets_and_start_doctags_for_corpusfile(corpus_file, self.workers)
+        #     kwargs['offsets'] = offsets
+        #     kwargs['start_doctags'] = start_doctags
+
+        super(Asm2Vec, self).train(
+            sentences=documents, corpus_file=corpus_file, total_examples=total_examples, total_words=total_words,
+            epochs=epochs, start_alpha=start_alpha, end_alpha=end_alpha, word_count=word_count,
+            queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)
+
+    # @classmethod
+    # def _get_offsets_and_start_doctags_for_corpusfile(cls, corpus_file, workers):
+    #     """Get offset and initial document tag in a corpus_file for each worker.
+
+    #     Firstly, approximate offsets are calculated based on number of workers and corpus_file size.
+    #     Secondly, for each approximate offset we find the maximum offset which points to the beginning of line and
+    #     less than approximate offset.
+
+    #     Parameters
+    #     ----------
+    #     corpus_file : str
+    #         Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.
+    #     workers : int
+    #         Number of workers.
+
+    #     Returns
+    #     -------
+    #     list of int, list of int
+    #         Lists with offsets and document tags with length = number of workers.
+    #     """
+    #     corpus_file_size = os.path.getsize(corpus_file)
+    #     approx_offsets = [int(corpus_file_size // workers * i) for i in range(workers)]
+    #     offsets = []
+    #     start_doctags = []
+
+    #     with utils.open(corpus_file, mode='rb') as fin:
+    #         curr_offset_idx = 0
+    #         prev_filepos = 0
+
+    #         for line_no, line in enumerate(fin):
+    #             if curr_offset_idx == len(approx_offsets):
+    #                 break
+
+    #             curr_filepos = prev_filepos + len(line)
+    #             while curr_offset_idx != len(approx_offsets) and approx_offsets[curr_offset_idx] < curr_filepos:
+    #                 offsets.append(prev_filepos)
+    #                 start_doctags.append(line_no)
+
+    #                 curr_offset_idx += 1
+
+    #             prev_filepos = curr_filepos
+
+    #     return offsets, start_doctags
+
+    def _raw_word_count(self, job):
+        """Get the number of words in a given job.
+
+        Parameters
+        ----------
+        job : iterable of list of :class:`~gensim.models.asm2vec.Function`
+            Corpus chunk.
+
+        Returns
+        -------
+        int
+            Number of raw words in the corpus chunk.
+
+        """
+        return sum(len(sentence.words) for sentence in job)
+
+    def estimated_lookup_memory(self):
+        """Get estimated memory for tag lookup, 0 if using pure int tags.
+
+        Returns
+        -------
+        int
+            The estimated RAM required to look up a tag in bytes.
+
+        """
+        return 60 * len(self.docvecs.offset2doctag) + 140 * len(self.docvecs.doctags)
+
+    def infer_vector(self, function, alpha=None, min_alpha=None, epochs=None, steps=None):
+        """Infer a vector for given post-bulk training document.
+
+        Notes
+        -----
+        Subsequent calls to this function may infer different representations for the same document.
+        For a more stable representation, increase the number of steps to assert a stricket convergence.
+
+        Parameters
+        ----------
+        doc_words : list of str
+            A document for which the vector representation will be inferred.
+        alpha : float, optional
+            The initial learning rate. If unspecified, value from model initialization will be reused.
+        min_alpha : float, optional
+            Learning rate will linearly drop to `min_alpha` over all inference epochs. If unspecified,
+            value from model initialization will be reused.
+        epochs : int, optional
+            Number of times to train the new document. Larger values take more time, but may improve
+            quality and run-to-run stability of inferred vectors. If unspecified, the `epochs` value
+            from model initialization will be reused.
+        steps : int, optional, deprecated
+            Previous name for `epochs`, still available for now for backward compatibility: if
+            `epochs` is unspecified but `steps` is, the `steps` value will be used.
+
+        Returns
+        -------
+        np.ndarray
+            The inferred paragraph vector for the new document.
+
+        """
+        if not isinstance(function, Function):
+            raise TypeError("Parameter function of infer_vector() must be a Function")
+
+        alpha = alpha or self.alpha
+        min_alpha = min_alpha or self.min_alpha
+        epochs = epochs or steps or self.epochs
+
+        doctag_vectors, doctag_locks = self.trainables.get_doctag_trainables(function, self.docvecs.vector_size)
+        doctag_indexes = [0]
+        work = zeros(self.trainables.layer1_size, dtype=REAL)
+        if not self.sg:
+            neu1 = matutils.zeros_aligned(self.trainables.layer1_size, dtype=REAL)
+        tmp_opnds =  matutils.zeros_aligned(self.wv.vector_size, dtype=REAL)
+
+        alpha_delta = (alpha - min_alpha) / max(epochs - 1, 1)
+
+        for i in range(epochs):
+            train_document_dm_asm2vec(
+                self, function, doctag_indexes, alpha, work, neu1, tmp_opnds,
+                learn_words=False, learn_hidden=False, doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
+            )
+            alpha -= alpha_delta
+
+        return doctag_vectors[0]
+
+    def infer_vector_multiple(self, functions_list, alpha=None, min_alpha=None, epochs=None, steps=None):
+        """Infer a vector for given post-bulk training document.
+
+        Notes
+        -----
+        Subsequent calls to this function may infer different representations for the same document.
+        For a more stable representation, increase the number of steps to assert a stricket convergence.
+
+        Parameters
+        ----------
+        doc_words : list of list of str
+            A list of documents for which the vector representation will be inferred.
+        alpha : float, optional
+            The initial learning rate. If unspecified, value from model initialization will be reused.
+        min_alpha : float, optional
+            Learning rate will linearly drop to `min_alpha` over all inference epochs. If unspecified,
+            value from model initialization will be reused.
+        epochs : int, optional
+            Number of times to train the new document. Larger values take more time, but may improve
+            quality and run-to-run stability of inferred vectors. If unspecified, the `epochs` value
+            from model initialization will be reused.
+        steps : int, optional, deprecated
+            Previous name for `epochs`, still available for now for backward compatibility: if
+            `epochs` is unspecified but `steps` is, the `steps` value will be used.
+
+        Returns
+        -------
+        np.ndarray
+            The inferred paragraph vector for the new document.
+
+        """
+        print("\nAM: infer_vector_multiple")
+
+        initialized_t = False
+        if not isinstance(functions_list, list):
+            raise TypeError("functions_list must be a list (not %s)" % type(functions_list))
+
+        for function in functions_list:    
+            if not isinstance(function, Function):
+                raise TypeError("Parameter function of infer_vector() must be a Function")
+
+            alpha_t = alpha or self.alpha
+            min_alpha_t = min_alpha or self.min_alpha
+            epochs = epochs or steps or self.epochs
+
+            if not initialized_t:
+                initialized_t = True
+                doctag_vectors, doctag_locks = self.trainables.get_doctag_trainables(function, self.docvecs.vector_size)
+                print("Initialized doctag_vectors: ", doctag_vectors[0])
+            doctag_indexes = [0]
+            work = zeros(self.trainables.layer1_size, dtype=REAL)
+
+            if not self.sg:
+                neu1 = matutils.zeros_aligned(self.trainables.layer1_size, dtype=REAL)
+            tmp_opnds =  matutils.zeros_aligned(self.wv.vector_size, dtype=REAL)
+
+            alpha_delta = (alpha_t - min_alpha_t) / max(epochs - 1, 1)
+
+            for i in range(epochs):
+                train_document_dm_asm2vec(
+                    self, function, doctag_indexes, alpha_t, work, neu1, tmp_opnds,
+                    learn_words=False, learn_hidden=False, doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
+                )
+                alpha_t -= alpha_delta
+            
+            print("Trained doctag_vectors: ", doctag_vectors[0])
+            print("Error: ", work)
+
+
+        return doctag_vectors[0]
+
+    def __getitem__(self, tag):
+        """Get the vector representation of (possible multi-term) tag.
+
+        Parameters
+        ----------
+        tag : {str, int, list of str, list of int}
+            The tag (or tags) to be looked up in the model.
+
+        Returns
+        -------
+        np.ndarray
+            The vector representations of each tag as a matrix (will be 1D if `tag` was a single tag)
+
+        """
+        if isinstance(tag, string_types + integer_types + (integer,)):
+            if tag not in self.wv.vocab:
+                return self.docvecs[tag]
+            return self.wv[tag]
+        return vstack([self[i] for i in tag])
+
+    def __str__(self):
+        """Abbreviated name reflecting major configuration parameters.
+
+        Returns
+        -------
+        str
+            Human readable representation of the models internal state.
+
+        """
+        segments = []
+        if self.comment:
+            segments.append('"%s"' % self.comment)
+
+        if self.cbow_mean:
+            segments.append('asm2vec/m')
+        else:
+            segments.append('asm2vec/s')
+
+        segments.append('d%d' % self.docvecs.vector_size)  # dimensions
+        if self.negative:
+            segments.append('n%d' % self.negative)  # negative samples
+        # if self.hs:
+        #     segments.append('hs')
+        
+        segments.append('w%d' % self.window)  # window size, when relevant
+        if self.vocabulary.min_count > 1:
+            segments.append('mc%d' % self.vocabulary.min_count)
+        if self.vocabulary.sample > 0:
+            segments.append('s%g' % self.vocabulary.sample)
+        if self.workers > 1:
+            segments.append('t%d' % self.workers)
+        return '%s(%s)' % (self.__class__.__name__, ','.join(segments))
+
+    def delete_temporary_training_data(self, keep_doctags_vectors=True, keep_inference=True):
+        """Discard parameters that are used in training and score. Use if you're sure you're done training a model.
+
+        Parameters
+        ----------
+        keep_doctags_vectors : bool, optional
+            Set to False if you don't want to save doctags vectors. In this case you will not be able to use
+            :meth:`~gensim.models.keyedvectors.Doc2VecKeyedVectors.most_similar`,
+            :meth:`~gensim.models.keyedvectors.Doc2VecKeyedVectors.similarity`, etc methods.
+        keep_inference : bool, optional
+            Set to False if you don't want to store parameters that are used for
+            :meth:`~gensim.models.asm2vec.Asm2Vec.infer_vector` method.
+
+        """
+        if not keep_inference:
+            if hasattr(self.trainables, 'syn1'):
+                del self.trainables.syn1
+            if hasattr(self.trainables, 'syn1neg'):
+                del self.trainables.syn1neg
+            if hasattr(self.trainables, 'vectors_lockf'):
+                del self.trainables.vectors_lockf
+        self.model_trimmed_post_training = True
+        if self.docvecs and hasattr(self.docvecs, 'vectors_docs') and not keep_doctags_vectors:
+            del self.docvecs.vectors_docs
+        if self.docvecs and hasattr(self.trainables, 'vectors_docs_lockf'):
+            del self.trainables.vectors_docs_lockf
+
+    def save_word2vec_format(self, fname, doctag_vec=False, word_vec=True, prefix='*dt_', fvocab=None, binary=False):
+        """Store the input-hidden weight matrix in the same format used by the original C word2vec-tool.
+
+        Parameters
+        ----------
+        fname : str
+            The file path used to save the vectors in.
+        doctag_vec : bool, optional
+            Indicates whether to store document vectors.
+        word_vec : bool, optional
+            Indicates whether to store word vectors.
+        prefix : str, optional
+            Uniquely identifies doctags from word vocab, and avoids collision in case of repeated string in doctag
+            and word vocab.
+        fvocab : str, optional
+            Optional file path used to save the vocabulary.
+        binary : bool, optional
+            If True, the data will be saved in binary word2vec format, otherwise - will be saved in plain text.
+
+        """
+        total_vec = len(self.wv.vocab) + len(self.docvecs)
+        write_first_line = False
+        # save word vectors
+        if word_vec:
+            if not doctag_vec:
+                total_vec = len(self.wv.vocab)
+            self.wv.save_word2vec_format(fname, fvocab, binary, total_vec)
+        # save document vectors
+        if doctag_vec:
+            if not word_vec:
+                total_vec = len(self.docvecs)
+                write_first_line = True
+            self.docvecs.save_word2vec_format(
+                fname, prefix=prefix, fvocab=fvocab, total_vec=total_vec,
+                binary=binary, write_first_line=write_first_line)
+
+    def init_sims(self, replace=False):
+        """Pre-compute L2-normalized vectors.
+
+        Parameters
+        ----------
+        replace : bool
+            If True - forget the original vectors and only keep the normalized ones to saved RAM (also you can't
+            continue training if call it with `replace=True`).
+
+        """
+        self.docvecs.init_sims(replace=replace)
+
+    @classmethod
+    def load(cls, *args, **kwargs):
+        """Load a previously saved :class:`~gensim.models.asm2vec.Asm2Vec` model.
+
+        Parameters
+        ----------
+        fname : str
+            Path to the saved file.
+        *args : object
+            Additional arguments, see `~gensim.models.base_any2vec.BaseWordEmbeddingsModel.load`.
+        **kwargs : object
+            Additional arguments, see `~gensim.models.base_any2vec.BaseWordEmbeddingsModel.load`.
+
+        See Also
+        --------
+        :meth:`~gensim.models.asm2vec.Asm2Vec.save`
+            Save :class:`~gensim.models.asm2vec.Asm2Vec` model.
+
+        Returns
+        -------
+        :class:`~gensim.models.asm2vec.Asm2Vec`
+            Loaded model.
+
+        """
+        try:
+            return super(Asm2Vec, cls).load(*args, **kwargs)
+        except AttributeError:
+            logger.info('Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.')
+            from gensim.models.deprecated.doc2vec import load_old_doc2vec
+            return load_old_doc2vec(*args, **kwargs)
+
+    def estimate_memory(self, vocab_size=None, report=None):
+        """Estimate required memory for a model using current settings.
+
+        Parameters
+        ----------
+        vocab_size : int, optional
+            Number of raw words in the vocabulary.
+        report : dict of (str, int), optional
+            A dictionary from string representations of the **specific** model's memory consuming members
+            to their size in bytes.
+
+        Returns
+        -------
+        dict of (str, int), optional
+            A dictionary from string representations of the model's memory consuming members to their size in bytes.
+            Includes members from the base classes as well as weights and tag lookup memory estimation specific to the
+            class.
+
+        """
+        report = report or {}
+        report['doctag_lookup'] = self.estimated_lookup_memory()
+        report['doctag_syn0'] = self.docvecs.count * self.vector_size * dtype(REAL).itemsize
+        return super(Asm2Vec, self).estimate_memory(vocab_size, report=report)
+
+    def build_vocab(self, documents=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False,
+                    trim_rule=None, **kwargs):
+        """Build vocabulary from a sequence of documents (can be a once-only generator stream).
+
+        Parameters
+        ----------
+        documents : iterable of list of :class:`~gensim.models.asm2vec.Function`, optional
+            Can be simply a list of :class:`~gensim.models.asm2vec.Function` elements, but for larger corpora,
+            consider an iterable that streams the documents directly from disk/network.
+            See :class:`~gensim.models.doc2vec.TaggedBrownCorpus` or :class:`~gensim.models.doc2vec.TaggedLineDocument`
+        corpus_file : str, optional
+            Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.
+            You may use this argument instead of `documents` to get performance boost. Only one of `documents` or
+            `corpus_file` arguments need to be passed (not both of them). Documents' tags are assigned automatically
+            and are equal to a line number, as in :class:`~gensim.models.doc2vec.TaggedLineDocument`.
+        update : bool
+            If true, the new words in `documents` will be added to model's vocab.
+        progress_per : int
+            Indicates how many words to process before showing/updating the progress.
+        keep_raw_vocab : bool
+            If not true, delete the raw vocabulary after the scaling is done and free up RAM.
+        trim_rule : function, optional
+            Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,
+            be trimmed away, or handled using the default (discard if word count < min_count).
+            Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),
+            or a callable that accepts parameters (word, count, min_count) and returns either
+            :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.
+            The rule, if given, is only used to prune vocabulary during current method call and is not stored as part
+            of the model.
+
+            The input parameters are of the following types:
+                * `word` (str) - the word we are examining
+                * `count` (int) - the word's frequency count in the corpus
+                * `min_count` (int) - the minimum count threshold.
+
+        **kwargs
+            Additional key word arguments passed to the internal vocabulary construction.
+
+        """
+        total_words, corpus_count = self.vocabulary.scan_vocab(
+            documents=documents, corpus_file=corpus_file, docvecs=self.docvecs,
+            progress_per=progress_per, trim_rule=trim_rule
+        )
+        self.corpus_count = corpus_count
+        self.corpus_total_words = total_words
+        report_values = self.vocabulary.prepare_vocab(
+            self.hs, self.negative, self.wv, update=update, keep_raw_vocab=keep_raw_vocab, trim_rule=trim_rule,
+            **kwargs)
+
+        report_values['memory'] = self.estimate_memory(vocab_size=report_values['num_retained_words'])
+        self.trainables.prepare_weights(
+            self.hs, self.negative, self.wv, self.docvecs, update=update)
+
+    def build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False):
+        """Build vocabulary from a dictionary of word frequencies.
+
+        Build model vocabulary from a passed dictionary that contains a (word -> word count) mapping.
+        Words must be of type unicode strings.
+
+        Parameters
+        ----------
+        word_freq : dict of (str, int)
+            Word <-> count mapping.
+        keep_raw_vocab : bool, optional
+            If not true, delete the raw vocabulary after the scaling is done and free up RAM.
+        corpus_count : int, optional
+            Even if no corpus is provided, this argument can set corpus_count explicitly.
+        trim_rule : function, optional
+            Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,
+            be trimmed away, or handled using the default (discard if word count < min_count).
+            Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),
+            or a callable that accepts parameters (word, count, min_count) and returns either
+            :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.
+            The rule, if given, is only used to prune vocabulary during
+            :meth:`~gensim.models.asm2vec.Asm2Vec.build_vocab` and is not stored as part of the model.
+
+            The input parameters are of the following types:
+                * `word` (str) - the word we are examining
+                * `count` (int) - the word's frequency count in the corpus
+                * `min_count` (int) - the minimum count threshold.
+
+        update : bool, optional
+            If true, the new provided words in `word_freq` dict will be added to model's vocab.
+
+        """
+        logger.info("Processing provided word frequencies")
+        # Instead of scanning text, this will assign provided word frequencies dictionary(word_freq)
+        # to be directly the raw vocab
+        raw_vocab = word_freq
+        logger.info(
+            "collected %i different raw word, with total frequency of %i",
+            len(raw_vocab), sum(itervalues(raw_vocab))
+        )
+
+        # Since no documents are provided, this is to control the corpus_count
+        self.corpus_count = corpus_count or 0
+        self.vocabulary.raw_vocab = raw_vocab
+
+        # trim by min_count & precalculate downsampling
+        report_values = self.vocabulary.prepare_vocab(
+            self.hs, self.negative, self.wv, keep_raw_vocab=keep_raw_vocab,
+            trim_rule=trim_rule, update=update)
+        report_values['memory'] = self.estimate_memory(vocab_size=report_values['num_retained_words'])
+        self.trainables.prepare_weights(
+            self.hs, self.negative, self.wv, self.docvecs, update=update)
+
+
+def _note_doctag(key, document_length, docvecs):
+    """Note a document tag during initial corpus scan, for structure sizing."""
+    if isinstance(key, integer_types + (integer,)):
+        docvecs.max_rawint = max(docvecs.max_rawint, key)
+    else:
+        if key in docvecs.doctags:
+            docvecs.doctags[key] = docvecs.doctags[key].repeat(document_length)
+        else:
+            docvecs.doctags[key] = Doctag(len(docvecs.offset2doctag), document_length, 1)
+            docvecs.offset2doctag.append(key)
+    docvecs.count = docvecs.max_rawint + 1 + len(docvecs.offset2doctag)
+
+
+class Asm2VecVocab(Word2VecVocab):
+    """Vocabulary used by :class:`~gensim.models.asm2vec.Asm2Vec`.
+
+    This includes a mapping from words found in the corpus to their total frequency count.
+
+    """
+    def __init__(self, max_vocab_size=None, min_count=5, sample=1e-3, sorted_vocab=True, null_word=0, ns_exponent=0.75):
+        """
+
+        Parameters
+        ----------
+        max_vocab_size : int, optional
+            Maximum number of words in the Vocabulary. Used to limit the RAM during vocabulary building;
+            if there are more unique words than this, then prune the infrequent ones.
+            Every 10 million word types need about 1GB of RAM, set to `None` for no limit.
+        min_count : int
+            Words with frequency lower than this limit will be discarded from the vocabulary.
+        sample : float, optional
+            The threshold for configuring which higher-frequency words are randomly downsampled,
+            useful range is (0, 1e-5).
+        sorted_vocab : bool
+            If True, sort the vocabulary by descending frequency before assigning word indexes.
+        null_word : {0, 1}
+            If True, a null pseudo-word will be created for padding when using concatenative L1 (run-of-words).
+            This word is only ever input – never predicted – so count, huffman-point, etc doesn't matter.
+        ns_exponent : float, optional
+            The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion
+            to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more
+            than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.
+            More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that
+            other values may perform better for recommendation applications.
+
+        """
+        super(Asm2VecVocab, self).__init__(
+            max_vocab_size=max_vocab_size, min_count=min_count, sample=sample,
+            sorted_vocab=sorted_vocab, null_word=null_word, ns_exponent=ns_exponent)
+
+    def _scan_vocab(self, documents, docvecs, progress_per, trim_rule):
+        document_no = -1
+        total_words = 0
+        min_reduce = 1
+        interval_start = default_timer() - 0.00001  # guard against next sample being identical
+        interval_count = 0
+        checked_string_types = 0
+        vocab = defaultdict(int)
+        for document_no, document in enumerate(documents):
+            if not checked_string_types:
+                if isinstance(document.get_tokens(), string_types):
+                    logger.warning(
+                        "Each 'words' should be a list of words (usually unicode strings). "
+                        "First 'words' here is instead plain %s.",
+                        type(document.get_tokens())
+                    )
+                checked_string_types += 1
+            if document_no % progress_per == 0:
+                interval_rate = (total_words - interval_count) / (default_timer() - interval_start)
+                logger.info(
+                    "PROGRESS: at example #%i, processed %i words (%i/s), %i word types, %i tags",
+                    document_no, total_words, interval_rate, len(vocab), docvecs.count
+                )
+                interval_start = default_timer()
+                interval_count = total_words
+            document_length = len(document.get_tokens())
+
+            for tag in document.tags:
+                _note_doctag(tag, document_length, docvecs)
+
+            for word in document.get_tokens():
+                vocab[word] += 1
+            total_words += len(document.get_tokens())
+
+            if self.max_vocab_size and len(vocab) > self.max_vocab_size:
+                utils.prune_vocab(vocab, min_reduce, trim_rule=trim_rule)
+                min_reduce += 1
+
+        corpus_count = document_no + 1
+        self.raw_vocab = vocab
+        return total_words, corpus_count
+
+    def scan_vocab(self, documents=None, corpus_file=None, docvecs=None, progress_per=10000, trim_rule=None):
+        """Create the models Vocabulary: A mapping from unique words in the corpus to their frequency count.
+
+        Parameters
+        ----------
+        documents : iterable of :class:`~gensim.models.asm2vec.Function`, optional
+            The tagged documents used to create the vocabulary. Their tags can be either str tokens or ints (faster).
+        corpus_file : str, optional
+            Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.
+            You may use this argument instead of `documents` to get performance boost. Only one of `documents` or
+            `corpus_file` arguments need to be passed (not both of them).
+        docvecs : list of :class:`~gensim.models.keyedvectors.Doc2VecKeyedVectors`
+            The vector representations of the documents in our corpus. Each of them has a size == `vector_size`.
+        progress_per : int
+            Progress will be logged every `progress_per` documents.
+        trim_rule : function, optional
+            Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,
+            be trimmed away, or handled using the default (discard if word count < min_count).
+            Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),
+            or a callable that accepts parameters (word, count, min_count) and returns either
+            :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.
+            The rule, if given, is only used to prune vocabulary during
+            :meth:`~gensim.models.asm2vec.Asm2Vec.build_vocab` and is not stored as part of the model.
+
+            The input parameters are of the following types:
+                * `word` (str) - the word we are examining
+                * `count` (int) - the word's frequency count in the corpus
+                * `min_count` (int) - the minimum count threshold.
+
+        Returns
+        -------
+        (int, int)
+            Tuple of (Total words in the corpus, number of documents)
+
+        """
+        logger.info("collecting all words and their counts")
+        if corpus_file is not None:
+            documents = TaggedLineDocument(corpus_file)
+
+        total_words, corpus_count = self._scan_vocab(documents, docvecs, progress_per, trim_rule)
+
+        logger.info(
+            "collected %i word types and %i unique tags from a corpus of %i examples and %i words",
+            len(self.raw_vocab), docvecs.count, corpus_count, total_words
+        )
+
+        return total_words, corpus_count
+
+    def indexed_doctags(self, doctag_tokens, docvecs):
+        """Get the indexes and backing-arrays used in training examples.
+
+        Parameters
+        ----------
+        doctag_tokens : list of {str, int}
+            A list of tags for which we want the index.
+        docvecs : list of :class:`~gensim.models.keyedvectors.Doc2VecKeyedVectors`
+            Vector representations of the documents in the corpus. Each vector has size == `vector_size`
+
+        Returns
+        -------
+        list of int
+            Indices of the provided tag keys.
+
+        """
+        return [
+            Doc2VecKeyedVectors._int_index(index, docvecs.doctags, docvecs.max_rawint)
+            for index in doctag_tokens if self._tag_seen(index, docvecs)]
+
+    def _tag_seen(self, index, docvecs):
+        """Whether or not the tag exists in our Vocabulary.
+
+        Parameters
+        ----------
+        index : {str, int}
+            The tag to be checked.
+        docvecs : :class:`~gensim.models.keyedvectors.Doc2VecKeyedVectors`
+            Vector representations of the documents in the corpus. Each vector has size == `vector_size`
+
+        Returns
+        -------
+        bool
+            Whether or not the passed tag exists in our vocabulary.
+
+        """
+        if isinstance(index, integer_types + (integer,)):
+            return index < docvecs.count
+        else:
+            return index in docvecs.doctags
+
+
+class Asm2VecTrainables(Word2VecTrainables):
+    """Represents the inner shallow neural network used to train :class:`~gensim.models.asm2vec.Asm2Vec`."""
+    def __init__(self, vector_size=100, seed=1, hashfxn=hash, window=5):
+        super(Asm2VecTrainables, self).__init__(
+            vector_size=vector_size, seed=seed, hashfxn=hashfxn)
+
+        self.layer1_size = 2 * vector_size
+
+    def prepare_weights(self, hs, negative, wv, docvecs, update=False):
+        """Build tables and model weights based on final vocabulary settings."""
+        # set initial input/projection and hidden weights
+        if not update:
+            self.reset_weights(hs, negative, wv, docvecs)
+        else:
+            self.update_weights(hs, negative, wv)
+
+    def reset_weights(self, hs, negative, wv, docvecs, vocabulary=None):
+        super(Asm2VecTrainables, self).reset_weights(hs, negative, wv)
+        self.reset_doc_weights(docvecs)
+
+    def reset_doc_weights(self, docvecs):
+        length = max(len(docvecs.doctags), docvecs.count)
+        if docvecs.mapfile_path:
+            docvecs.vectors_docs = np_memmap(
+                docvecs.mapfile_path + '.vectors_docs', dtype=REAL, mode='w+', shape=(length, docvecs.vector_size)
+            )
+            self.vectors_docs_lockf = np_memmap(
+                docvecs.mapfile_path + '.vectors_docs_lockf', dtype=REAL, mode='w+', shape=(length,)
+            )
+            self.vectors_docs_lockf.fill(1.0)
+        else:
+            docvecs.vectors_docs = empty((length, docvecs.vector_size), dtype=REAL)
+            self.vectors_docs_lockf = ones((length,), dtype=REAL)  # zeros suppress learning
+
+        for i in range(length):
+            # construct deterministic seed from index AND model seed
+            seed = "%d %s" % (
+                self.seed, Doc2VecKeyedVectors._index_to_doctag(i, docvecs.offset2doctag, docvecs.max_rawint))
+            docvecs.vectors_docs[i] = self.seeded_vector(seed, docvecs.vector_size)
+
+    def get_doctag_trainables(self, function, vector_size):
+        doctag_vectors = zeros((1, vector_size), dtype=REAL)
+        doctag_vectors[0] = self.seeded_vector(' '.join(function.get_tokens()), vector_size)
+        doctag_locks = ones(1, dtype=REAL)
+        return doctag_vectors, doctag_locks
diff -Nru ../gensim-3.8.3/gensim/models/doc2vec.py ./gensim/models/doc2vec.py
--- ../gensim-3.8.3/gensim/models/doc2vec.py	2020-05-03 01:52:10.000000000 +0200
+++ ./gensim/models/doc2vec.py	2022-08-03 16:25:12.000000000 +0200
@@ -149,6 +149,29 @@
         return '%s(%s, %s)' % (self.__class__.__name__, self.words, self.tags)
 
 
+# class Instruction(namedtuple('Instruction', 'operator operands')):
+# """Represents an instruction along with an operator and operands."""
+
+#     def __str__(self):
+#         """Human readable representation of the object's state, used for debugging.
+
+#         Returns
+#         -------
+#         str
+#            Human readable representation of the object's state (words and tags).
+
+#         """
+#         return '%s(%s, %s)' % (self.__class__.__name__, self.operator, self.operands)
+
+#     def get_tokens(self):
+#         return [self.operator] + self.operands
+
+
+# class Function(TaggedDocument):
+#     """Placeholder"""
+#     pass
+
+
 # for compatibility
 @deprecated("Class will be removed in 4.0.0, use TaggedDocument instead")
 class LabeledSentence(TaggedDocument):
@@ -334,17 +357,22 @@
         self.dm_concat = int(dm_concat)
         self.dm_tag_count = int(dm_tag_count)
 
+        # print("[D] Doc2VecVocab")
         kwargs['null_word'] = dm_concat
         vocabulary_keys = ['max_vocab_size', 'min_count', 'sample', 'sorted_vocab', 'null_word', 'ns_exponent']
         vocabulary_kwargs = dict((k, kwargs[k]) for k in vocabulary_keys if k in kwargs)
         self.vocabulary = Doc2VecVocab(**vocabulary_kwargs)
 
+        # print("[D] trainables")
         trainables_keys = ['seed', 'hashfxn', 'window']
         trainables_kwargs = dict((k, kwargs[k]) for k in trainables_keys if k in kwargs)
         self.trainables = Doc2VecTrainables(
             dm=dm, dm_concat=dm_concat, dm_tag_count=dm_tag_count,
             vector_size=self.vector_size, **trainables_kwargs)
 
+        self.quick_inference = False
+
+        # print("[D] keyedvectors")
         self.wv = Word2VecKeyedVectors(self.vector_size)
         self.docvecs = docvecs or Doc2VecKeyedVectors(self.vector_size, docvecs_mapfile)
 
@@ -356,7 +384,11 @@
                 raise TypeError("You must pass string as the corpus_file argument.")
             elif isinstance(documents, GeneratorType):
                 raise TypeError("You can't pass a generator as the documents argument. Try a sequence.")
+
+            print("[D] build_vocab")
             self.build_vocab(documents=documents, corpus_file=corpus_file, trim_rule=trim_rule)
+
+            print("[D] train")
             self.train(
                 documents=documents, corpus_file=corpus_file, total_examples=self.corpus_count,
                 total_words=self.corpus_total_words, epochs=self.epochs, start_alpha=self.alpha,
@@ -408,6 +440,26 @@
         self.docvecs.offset2doctag = other_model.docvecs.offset2doctag
         self.trainables.reset_weights(self.hs, self.negative, self.wv, self.docvecs)
 
+    def reset_model_for_fast_inference(self, other_model, docvecs_count):
+        """Copy shareable data structures from another (possibly pre-trained) model.
+
+        Parameters
+        ----------
+        other_model : :class:`~gensim.models.asm2vec.Asm2Vec`
+            Other model whose internal data structures will be copied over to the current object.
+
+        """
+        self.wv = other_model.wv
+        self.vocabulary.cum_table = other_model.vocabulary.cum_table
+        self.corpus_count = other_model.corpus_count
+        self.docvecs.count = docvecs_count
+        self.docvecs.doctags = other_model.docvecs.doctags
+        self.docvecs.offset2doctag = other_model.docvecs.offset2doctag
+        self.trainables = other_model.trainables
+        self.trainables.reset_doc_weights(self.docvecs)
+        self.quick_inference = True
+        return
+
     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,
                         total_examples=None, total_words=None, offsets=None, start_doctags=None, **kwargs):
         work, neu1 = thread_private_mem
@@ -460,20 +512,41 @@
             doctag_vectors = self.docvecs.vectors_docs
             doctag_locks = self.trainables.vectors_docs_lockf
             if self.sg:
-                tally += train_document_dbow(
-                    self, doc.words, doctag_indexes, alpha, work, train_words=self.dbow_words,
-                    doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
-                )
+                if self.quick_inference:
+                    tally += train_document_dbow(
+                        self, doc.words, doctag_indexes, alpha, work, train_words=self.dbow_words,
+                        learn_words=False, learn_hidden=False,
+                        doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
+                    )
+                else:
+                    tally += train_document_dbow(
+                        self, doc.words, doctag_indexes, alpha, work, train_words=self.dbow_words,
+                        doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
+                    )
             elif self.dm_concat:
-                tally += train_document_dm_concat(
-                    self, doc.words, doctag_indexes, alpha, work, neu1,
-                    doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
-                )
+                if self.quick_inference:
+                    tally += train_document_dm_concat(
+                        self, doc.words, doctag_indexes, alpha, work, neu1,
+                        learn_words=False, learn_hidden=False,
+                        doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
+                    )
+                else:
+                    tally += train_document_dm_concat(
+                        self, doc.words, doctag_indexes, alpha, work, neu1,
+                        doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
+                    )
             else:
-                tally += train_document_dm(
-                    self, doc.words, doctag_indexes, alpha, work, neu1,
-                    doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
-                )
+                if self.quick_inference:
+                    tally += train_document_dm(
+                        self, doc.words, doctag_indexes, alpha, work, neu1,
+                        learn_words=False, learn_hidden=False,
+                        doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
+                    )
+                else:
+                    tally += train_document_dm(
+                        self, doc.words, doctag_indexes, alpha, work, neu1,
+                        doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
+                    )
         return tally, self._raw_word_count(job)
 
     def train(self, documents=None, corpus_file=None, total_examples=None, total_words=None,
@@ -693,6 +766,90 @@
 
         return doctag_vectors[0]
 
+    def infer_vector_multiple(self, doc_words_list, alpha=None, min_alpha=None, epochs=None, steps=None):
+        """Infer a vector for given post-bulk training document.
+
+        Notes
+        -----
+        Subsequent calls to this function may infer different representations for the same document.
+        For a more stable representation, increase the number of steps to assert a stricket convergence.
+
+        Parameters
+        ----------
+        doc_words : list of list of str
+            A list of documents for which the vector representation will be inferred.
+        alpha : float, optional
+            The initial learning rate. If unspecified, value from model initialization will be reused.
+        min_alpha : float, optional
+            Learning rate will linearly drop to `min_alpha` over all inference epochs. If unspecified,
+            value from model initialization will be reused.
+        epochs : int, optional
+            Number of times to train the new document. Larger values take more time, but may improve
+            quality and run-to-run stability of inferred vectors. If unspecified, the `epochs` value
+            from model initialization will be reused.
+        steps : int, optional, deprecated
+            Previous name for `epochs`, still available for now for backward compatibility: if
+            `epochs` is unspecified but `steps` is, the `steps` value will be used.
+
+        Returns
+        -------
+        np.ndarray
+            The inferred paragraph vector for the new document.
+
+        """
+        # print("\nAM: infer_vector_multiple")
+
+        initialized_t = False
+        if not isinstance(doc_words_list, list):
+            raise TypeError("doc_words_list must be a list (not %s)" % type(doc_words_list))
+
+        if len(doc_words_list) == 0:
+            return list()
+
+        for doc_words in doc_words_list:    
+            if isinstance(doc_words, string_types):
+                raise TypeError("Parameter doc_words of infer_vector() must be a list of strings (not a single string).")
+
+            alpha_t = alpha or self.alpha
+            min_alpha_t = min_alpha or self.min_alpha
+            epochs = epochs or steps or self.epochs
+
+            if not initialized_t:
+                initialized_t = True
+                doctag_vectors, doctag_locks = self.trainables.get_doctag_trainables(doc_words, self.docvecs.vector_size)
+                # print("Initialized doctag_vectors: ", doctag_vectors[0])
+
+            doctag_indexes = [0]
+            work = zeros(self.trainables.layer1_size, dtype=REAL)
+
+            if not self.sg:
+                neu1 = matutils.zeros_aligned(self.trainables.layer1_size, dtype=REAL)
+
+            alpha_delta = (alpha_t - min_alpha_t) / max(epochs - 1, 1)
+
+            for i in range(epochs):
+                if self.sg:
+                    train_document_dbow(
+                        self, doc_words, doctag_indexes, alpha_t, work,
+                        learn_words=False, learn_hidden=False, doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
+                    )
+                elif self.dm_concat:
+                    train_document_dm_concat(
+                        self, doc_words, doctag_indexes, alpha_t, work, neu1,
+                        learn_words=False, learn_hidden=False, doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
+                    )
+                else:
+                    train_document_dm(
+                        self, doc_words, doctag_indexes, alpha_t, work, neu1,
+                        learn_words=False, learn_hidden=False, doctag_vectors=doctag_vectors, doctag_locks=doctag_locks
+                    )
+                alpha_t -= alpha_delta
+            
+            # print("Trained doctag_vectors: ", doctag_vectors[0])
+            # print("Error: ", work)
+
+        return doctag_vectors[0]
+
     def __getitem__(self, tag):
         """Get the vector representation of (possible multi-term) tag.

diff -Nru ../gensim-3.8.3/gensim/models/doc2vec_inner.pxd ./gensim/models/doc2vec_inner.pxd
--- ../gensim-3.8.3/gensim/models/doc2vec_inner.pxd	2020-05-03 01:52:10.000000000 +0200
+++ ./gensim/models/doc2vec_inner.pxd	2022-08-03 16:25:12.000000000 +0200
@@ -18,6 +18,44 @@
 from word2vec_inner cimport REAL_t
 
 DEF MAX_DOCUMENT_LEN = 10000
+DEF MAX_TOKENS_LEN = MAX_DOCUMENT_LEN * 5
+
+
+cdef struct Asm2VecConfig:
+    # int hs,
+    int negative, sample, learn_doctags, learn_words, learn_hidden, train_words, cbow_mean
+    int document_len, doctag_len, window, expected_doctag_len, null_word_index, workers, docvecs_count
+
+    REAL_t *word_vectors
+    REAL_t *doctag_vectors
+    REAL_t *word_locks
+    REAL_t *doctag_locks
+    REAL_t *work
+    REAL_t *neu1
+    REAL_t *tmp_opnds
+    REAL_t alpha
+    int layer1_size, doctag_vector_size, word_vector_size
+
+    int codelens[MAX_DOCUMENT_LEN]
+    np.uint32_t indexes[MAX_DOCUMENT_LEN]
+    np.uint32_t doctag_indexes[MAX_DOCUMENT_LEN]
+    np.uint32_t window_indexes[MAX_DOCUMENT_LEN]
+    np.uint32_t reduced_windows[MAX_DOCUMENT_LEN]
+
+    np.uint32_t operators_idx[MAX_DOCUMENT_LEN]
+    np.uint32_t operands_idx[MAX_TOKENS_LEN]
+    np.uint32_t operands_len[MAX_DOCUMENT_LEN]
+    np.uint32_t operands_offset[MAX_DOCUMENT_LEN]
+
+    # For hierarchical softmax
+    REAL_t *syn1
+    np.uint32_t *points[MAX_DOCUMENT_LEN]
+    np.uint8_t *codes[MAX_DOCUMENT_LEN]
+
+    # For negative sampling
+    REAL_t *syn1neg
+    np.uint32_t *cum_table
+    unsigned long long cum_table_len, next_random
 
 
 cdef struct Doc2VecConfig:
@@ -88,5 +126,9 @@
     const int layer1_size, const int vector_size, int learn_hidden) nogil
 
 
+cdef init_a2v_config(Asm2VecConfig *c, model, alpha, learn_doctags, learn_words, learn_hidden, train_words=*, work=*,
+                     neu1=*, tmp_opnds=*, word_vectors=*, word_locks=*, doctag_vectors=*, doctag_locks=*, docvecs_count=*)
+
+
 cdef init_d2v_config(Doc2VecConfig *c, model, alpha, learn_doctags, learn_words, learn_hidden, train_words=*, work=*,
                      neu1=*, word_vectors=*, word_locks=*, doctag_vectors=*, doctag_locks=*, docvecs_count=*)
diff -Nru ../gensim-3.8.3/gensim/models/doc2vec_inner.pyx ./gensim/models/doc2vec_inner.pyx
--- ../gensim-3.8.3/gensim/models/doc2vec_inner.pyx	2020-05-03 01:52:10.000000000 +0200
+++ ./gensim/models/doc2vec_inner.pyx	2022-08-03 16:25:12.000000000 +0200
@@ -15,6 +15,9 @@
 from numpy import zeros, float32 as REAL
 cimport numpy as np
 
+# NOTE: for debugging
+# np.import_array()
+
 from libc.string cimport memset, memcpy
 
 # scipy <= 0.15
@@ -27,6 +30,7 @@
 from word2vec_inner cimport bisect_left, random_int32, sscal, REAL_t, EXP_TABLE, our_dot, our_saxpy
 
 DEF MAX_DOCUMENT_LEN = 10000
+DEF MAX_TOKENS_LEN = MAX_DOCUMENT_LEN * 5
 
 cdef int ONE = 1
 cdef REAL_t ONEF = <REAL_t>1.0
@@ -220,6 +224,74 @@
     return next_random
 
 
+
+cdef init_a2v_config(Asm2VecConfig *c, model, alpha, learn_doctags, learn_words, learn_hidden,
+                     train_words=False, work=None, neu1=None, tmp_opnds=None, word_vectors=None, word_locks=None, doctag_vectors=None,
+                     doctag_locks=None, docvecs_count=0):
+    # c[0].hs = model.hs
+    c[0].negative = model.negative
+    c[0].sample = (model.vocabulary.sample != 0)
+    c[0].cbow_mean = model.cbow_mean
+    c[0].train_words = train_words
+    c[0].learn_doctags = learn_doctags
+    c[0].learn_words = learn_words
+    c[0].learn_hidden = learn_hidden
+    c[0].alpha = alpha
+    c[0].layer1_size = model.trainables.layer1_size
+    c[0].doctag_vector_size = model.docvecs.vector_size
+    c[0].word_vector_size = model.wv.vector_size
+    c[0].workers = model.workers
+    c[0].docvecs_count = docvecs_count
+
+    c[0].window = model.window
+    # c[0].expected_doctag_len = model.dm_tag_count
+    c[0].expected_doctag_len = 1
+
+    if '\0' in model.wv.vocab:
+        c[0].null_word_index = model.wv.vocab['\0'].index
+
+    # default vectors, locks from syn0/doctag_syn0
+    if word_vectors is None:
+       word_vectors = model.wv.vectors
+    c[0].word_vectors = <REAL_t *>(np.PyArray_DATA(word_vectors))
+    
+    if doctag_vectors is None:
+       doctag_vectors = model.docvecs.vectors_docs
+    c[0].doctag_vectors = <REAL_t *>(np.PyArray_DATA(doctag_vectors))
+    
+    if word_locks is None:
+       word_locks = model.trainables.vectors_lockf
+    c[0].word_locks = <REAL_t *>(np.PyArray_DATA(word_locks))
+    
+    if doctag_locks is None:
+       doctag_locks = model.trainables.vectors_docs_lockf
+    c[0].doctag_locks = <REAL_t *>(np.PyArray_DATA(doctag_locks))
+
+    # if c[0].hs:
+    #     c[0].syn1 = <REAL_t *>(np.PyArray_DATA(model.trainables.syn1))
+
+    if c[0].negative:
+        c[0].syn1neg = <REAL_t *>(np.PyArray_DATA(model.trainables.syn1neg))
+        c[0].cum_table = <np.uint32_t *>(np.PyArray_DATA(model.vocabulary.cum_table))
+        c[0].cum_table_len = len(model.vocabulary.cum_table)
+    if c[0].negative or c[0].sample:
+        c[0].next_random = (2**24) * model.random.randint(0, 2**24) + model.random.randint(0, 2**24)
+
+    # convert Python structures to primitive types, so we can release the GIL
+    if work is None:
+       work = zeros(model.trainables.layer1_size, dtype=REAL)
+    c[0].work = <REAL_t *>np.PyArray_DATA(work)
+    
+    if neu1 is None:
+       neu1 = zeros(model.trainables.layer1_size, dtype=REAL)
+    c[0].neu1 = <REAL_t *>np.PyArray_DATA(neu1)
+
+    if tmp_opnds is None:
+        tmp_opnds = zeros(model.wv.vector_size, dtype=REAL)
+    c[0].tmp_opnds = <REAL_t *>np.PyArray_DATA(tmp_opnds)
+
+
+
 cdef init_d2v_config(Doc2VecConfig *c, model, alpha, learn_doctags, learn_words, learn_hidden,
                      train_words=False, work=None, neu1=None, word_vectors=None, word_locks=None, doctag_vectors=None,
                      doctag_locks=None, docvecs_count=0):
@@ -366,6 +438,7 @@
     # release GIL & train on the document
     with nogil:
         for i in range(c.document_len):
+
             if c.train_words:  # simultaneous skip-gram wordvec-training
                 j = i - c.window + c.reduced_windows[i]
                 if j < 0:
@@ -401,6 +474,247 @@
     return result
 
 
+def train_document_dm_asm2vec(model, function, doctag_indexes, alpha, work=None, neu1=None, tmp_opnds=None,
+                      learn_doctags=True, learn_words=True, learn_hidden=True,
+                      word_vectors=None, word_locks=None, doctag_vectors=None, doctag_locks=None):
+    """Update distributed memory model ("PV-DM") by training on a single document.
+    This method implements the DM model with a projection (input) layer that is either the sum or mean of the context
+    vectors, depending on the model's `dm_mean` configuration field.
+
+    Called internally from :meth:`~gensim.models.doc2vec.Doc2Vec.train` and
+    :meth:`~gensim.models.doc2vec.Doc2Vec.infer_vector`.
+
+    Parameters
+    ----------
+    model : :class:`~gensim.models.doc2vec.Doc2Vec`
+        The model to train.
+    doc_words : list of str
+        The input document as a list of words to be used for training. Each word will be looked up in
+        the model's vocabulary.
+    doctag_indexes : list of int
+        Indices into `doctag_vectors` used to obtain the tags of the document.
+    alpha : float
+        Learning rate.
+    work : np.ndarray, optional
+        Private working memory for each worker.
+    neu1 : np.ndarray, optional
+        Private working memory for each worker.
+    learn_doctags : bool, optional
+        Whether the tag vectors should be updated.
+    learn_words : bool, optional
+        Word vectors will be updated exactly as per Word2Vec skip-gram training only if **both**
+        `learn_words` and `train_words` are set to True.
+    learn_hidden : bool, optional
+        Whether or not the weights of the hidden layer will be updated.
+    word_vectors : numpy.ndarray, optional
+        The vector representation for each word in the vocabulary. If None, these will be retrieved from the model.
+    word_locks : numpy.ndarray, optional
+        A learning lock factor for each weight in the hidden layer for words, value 0 completely blocks updates,
+        a value of 1 allows to update word-vectors.
+    doctag_vectors : numpy.ndarray, optional
+        Vector representations of the tags. If None, these will be retrieved from the model.
+    doctag_locks : numpy.ndarray, optional
+        The lock factors for each tag, same as `word_locks`, but for document-vectors.
+
+    Returns
+    -------
+    int
+        Number of words in the input document that were actually used for training.
+
+    """
+    cdef Asm2VecConfig c
+
+    cdef REAL_t count, inv_count_opnd, inv_count = 1.0
+    cdef int i, j, k, m, o, z
+    cdef int t, predict_word_index
+    cdef long result = 0
+
+    init_a2v_config(&c, model, alpha, learn_doctags, learn_words, learn_hidden, train_words=False,
+                    work=work, neu1=neu1, tmp_opnds=tmp_opnds, word_vectors=word_vectors, word_locks=word_locks,
+                    doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
+
+    # NOTE: Set the number of function's tags and instructions
+    c.doctag_len = <int>min(MAX_DOCUMENT_LEN, len(doctag_indexes))
+    c.document_len = <int>min(MAX_DOCUMENT_LEN, len(function.words))
+
+    # NOTE: dictionary of tokens
+    vlookup = model.wv.vocab
+
+    opnd_cnt = 0
+    for i, instruction in enumerate(function.words):
+
+        # 10,000
+        if i >= MAX_DOCUMENT_LEN:
+            break  # TODO: log warning, tally overflow?
+
+        # 50,000
+        if opnd_cnt >= MAX_TOKENS_LEN:
+            break  # TODO: log warning, tally overflow?
+
+        # INSTRUCTION.OPERATOR
+        predict_word = vlookup[instruction.operator] if instruction.operator in vlookup else None
+        if predict_word:
+            # TODO uncomment this to add sampling
+            # if c.sample and predict_word.sample_int < random_int32(&c.next_random):
+            #     print("sub-sampling - skipping operator")
+            c.operators_idx[i] = predict_word.index
+        else:
+            c.operators_idx[i] = c.null_word_index
+
+        # INSTRUCTION.OPERANDS
+        # Keep track of how many operands per instruction
+        opnd_cnt_i = 0
+        # ... and of the starting offset for the current instruction
+        opnd_offset_i = opnd_cnt
+        for operand in instruction.operands:
+
+            predict_word = vlookup[operand] if operand in vlookup else None
+            if predict_word:
+                c.operands_idx[opnd_cnt] = predict_word.index
+
+                opnd_cnt += 1
+                opnd_cnt_i += 1
+
+        c.operands_len[i] = opnd_cnt_i
+        c.operands_offset[i] = opnd_offset_i
+
+    # Get document_len random numbers to reduce the window size
+    # NOTE: How does it affect the results? 
+    # single randint() call avoids a big thread-sync slowdown
+    for i, item in enumerate(model.random.randint(0, c.window, c.document_len)):
+        c.reduced_windows[i] = item
+
+    # NOTE: Save the indexes for the function's tags in the c struct.
+    for i in range(c.doctag_len):
+        c.doctag_indexes[i] = doctag_indexes[i]
+        result += 1
+
+    # release GIL & train on the document
+    with nogil:
+
+        # NOTE: "i" is the index for the target instruction in the document
+        # NOTE: "j" and "k" are the bounds of the window.
+        for i in range(c.document_len):
+            j = i - c.window + c.reduced_windows[i]
+            if j < 0:
+                j = 0
+            k = i + c.window + 1 - c.reduced_windows[i]
+            if k > c.document_len:
+                k = c.document_len
+
+            # Define the predict_word_index
+            # i is the target instruction
+            for t in range(c.operands_len[i] + 1):
+                if t == c.operands_len[i]:
+                    # the target to predict is the operator
+                    predict_word_index = c.operators_idx[i]
+                else:
+                    # the target to predict is one of the operands
+                    predict_word_index = c.operands_idx[c.operands_offset[i] + t]
+
+                # compose l1 (in _neu1) & clear _work
+                memset(c.neu1, 0, c.layer1_size * cython.sizeof(REAL_t))
+                count = <REAL_t>0.0
+
+                # NOTE: "m" is the index of the instruction in the window
+                for m in range(j, k):
+                    if m == i:
+                        # target instruction, skip it
+                        continue
+                    else:
+                        count += ONEF
+
+                        # OPERATOR
+                        our_saxpy(&c.word_vector_size, &ONEF, &c.word_vectors[c.operators_idx[m] * c.word_vector_size], &ONE, c.neu1, &ONE)
+
+                        # OPERANDS
+                        memset(c.tmp_opnds, 0, c.word_vector_size * cython.sizeof(REAL_t))
+                        inv_count_opnd = <REAL_t>1.0
+
+                        for o in range(c.operands_len[m]):
+                            z = c.operands_idx[c.operands_offset[m] + o]
+                            our_saxpy(&c.word_vector_size, &ONEF, &c.word_vectors[z * c.word_vector_size], &ONE, c.tmp_opnds, &ONE)
+
+                        if c.operands_len[m] > 0:
+                            inv_count_opnd = ONEF/c.operands_len[m]
+
+                        # NOTE: sscal scales a vector by a constant.
+                        if c.cbow_mean:
+                            sscal(&c.word_vector_size, &inv_count_opnd, c.tmp_opnds, &ONE)  # (does this need BLAS-variants like saxpy?)
+                        
+                        # Add the operands vector (c.tmp_opnds) to the right-part of c.neu1
+                        # our_saxpy(&c.word_vector_size, &ONEF, c.tmp_opnds, &ONE, c.neu1+(c.word_vector_size * cython.sizeof(REAL_t)), &ONE)
+                        our_saxpy(&c.word_vector_size, &ONEF, c.tmp_opnds, &ONE, &c.neu1[c.word_vector_size], &ONE)
+                
+                # NOTE: update c.neu1 with the doctag vectors
+                for m in range(c.doctag_len):
+                    count += ONEF
+                    our_saxpy(&c.layer1_size, &ONEF, &c.doctag_vectors[c.doctag_indexes[m] * c.layer1_size], &ONE, c.neu1, &ONE)
+                
+                if count > (<REAL_t>0.5):
+                    inv_count = ONEF/count
+                
+                ## NOTE: sscal scales a vector by a constant.
+                if c.cbow_mean:
+                    sscal(&c.layer1_size, &inv_count, c.neu1, &ONE)  # (does this need BLAS-variants like saxpy?)
+
+                ## WORK --> ERROR
+                memset(c.work, 0, c.layer1_size * cython.sizeof(REAL_t))  # work to accumulate l1 error
+                
+                # NOTE: Hierarchical Softmax is not implemented
+                # if c.hs:
+                #     fast_document_dm_hs(c.points[i], c.codes[i], c.codelens[i], c.neu1, c.syn1, c.alpha, c.work,
+                #                         c.layer1_size, c.learn_hidden)
+                # NOTE: Use negative sampling
+                if c.negative:
+                    c.next_random = fast_document_dm_neg(c.negative, c.cum_table, c.cum_table_len, c.next_random,
+                                                         c.neu1, c.syn1neg, predict_word_index, c.alpha, c.work, c.layer1_size,
+                                                         c.learn_hidden)
+
+                # NOTE: If not using the AVG of the vectors, compute the mean of the error
+                if not c.cbow_mean:
+                    sscal(&c.layer1_size, &inv_count, c.work, &ONE)  # (does this need BLAS-variants like saxpy?)
+
+                # apply accumulated error in work
+                # NOTE: Check when doctag_locks are set.
+                if c.learn_doctags:
+                    for m in range(c.doctag_len):
+                        our_saxpy(&c.layer1_size, &c.doctag_locks[c.doctag_indexes[m]], c.work,
+                                  &ONE, &c.doctag_vectors[c.doctag_indexes[m] * c.layer1_size], &ONE)
+
+                # NOTE: learn the tags too
+                if c.learn_words:
+                    for m in range(j, k):
+                        if m == i:
+                            continue
+                        else:
+                            # OPERATOR
+                            our_saxpy(&c.word_vector_size, &c.word_locks[c.operators_idx[m]], c.work, &ONE,
+                                       &c.word_vectors[c.operators_idx[m]  * c.word_vector_size], &ONE)
+
+                            # OPERANDS
+                            
+                            memset(c.tmp_opnds, 0, c.word_vector_size * cython.sizeof(REAL_t))
+                            inv_count_opnd = <REAL_t>1.0
+                            # NOTE: copy the error relative the operands to c.tmp_opnds
+                            our_saxpy(&c.word_vector_size, &ONEF, &c.work[c.word_vector_size],
+                                    &ONE, c.tmp_opnds, &ONE)
+
+                            if c.operands_len[m] > 0:
+                                inv_count_opnd = ONEF/c.operands_len[m]
+
+                            # NOTE: sscal scales a vector by a constant.
+                            if not c.cbow_mean:
+                                sscal(&c.word_vector_size, &inv_count_opnd, c.tmp_opnds, &ONE)  # (does this need BLAS-variants like saxpy?)
+
+                            for o in range(c.operands_len[m]):
+                                z = c.operands_idx[c.operands_offset[m] + o]
+                                our_saxpy(&c.word_vector_size, &c.word_locks[z], c.tmp_opnds, &ONE,
+                                           &c.word_vectors[z * c.word_vector_size], &ONE)
+
+    return result
+
+
 def train_document_dm(model, doc_words, doctag_indexes, alpha, work=None, neu1=None,
                       learn_doctags=True, learn_words=True, learn_hidden=True,
                       word_vectors=None, word_locks=None, doctag_vectors=None, doctag_locks=None):
@@ -501,12 +815,12 @@
             # compose l1 (in _neu1) & clear _work
             memset(c.neu1, 0, c.layer1_size * cython.sizeof(REAL_t))
             count = <REAL_t>0.0
-            for m in range(j, k):
-                if m == i:
-                    continue
-                else:
-                    count += ONEF
-                    our_saxpy(&c.layer1_size, &ONEF, &c.word_vectors[c.indexes[m] * c.layer1_size], &ONE, c.neu1, &ONE)
+            # for m in range(j, k):
+            #     if m == i:
+            #         continue
+            #     else:
+            #         count += ONEF
+            #         our_saxpy(&c.layer1_size, &ONEF, &c.word_vectors[c.indexes[m] * c.layer1_size], &ONE, c.neu1, &ONE)
             for m in range(c.doctag_len):
                 count += ONEF
                 our_saxpy(&c.layer1_size, &ONEF, &c.doctag_vectors[c.doctag_indexes[m] * c.layer1_size], &ONE, c.neu1, &ONE)
@@ -530,13 +844,13 @@
                 for m in range(c.doctag_len):
                     our_saxpy(&c.layer1_size, &c.doctag_locks[c.doctag_indexes[m]], c.work,
                               &ONE, &c.doctag_vectors[c.doctag_indexes[m] * c.layer1_size], &ONE)
-            if c.learn_words:
-                for m in range(j, k):
-                    if m == i:
-                        continue
-                    else:
-                         our_saxpy(&c.layer1_size, &c.word_locks[c.indexes[m]], c.work, &ONE,
-                                   &c.word_vectors[c.indexes[m] * c.layer1_size], &ONE)
+            # if c.learn_words:
+            #     for m in range(j, k):
+            #         if m == i:
+            #             continue
+            #         else:
+            #              our_saxpy(&c.layer1_size, &c.word_locks[c.indexes[m]], c.work, &ONE,
+            #                        &c.word_vectors[c.indexes[m] * c.layer1_size], &ONE)
 
     return result
 
